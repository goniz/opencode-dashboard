# OpenAI Chat Completions API Reference

This document provides comprehensive information for implementing an OpenAI-compatible Chat Completions API.

## Overview

The Chat Completions API allows you to generate conversational responses using language models. It takes a series of messages as input and returns an AI-generated message as output.

## API Endpoint

```
POST /v1/chat/completions
```

## Request Format

### Required Parameters

- **`model`** (string): The name of the model to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-4o-mini`)
- **`messages`** (array): A list of message objects, where each object has:
  - **`role`** (string): The role of the messenger (`system`, `user`, `assistant`, or `tool`)
  - **`content`** (string): The content of the message

### Optional Parameters

- **`frequency_penalty`** (number): Penalizes tokens based on their frequency, reducing repetition. Range: -2.0 to 2.0
- **`logit_bias`** (object): Modifies likelihood of specified tokens with bias values
- **`logprobs`** (boolean): Returns log probabilities of output tokens if true
- **`top_logprobs`** (integer): Specifies the number of most likely tokens to return at each position
- **`max_tokens`** (integer): Sets the maximum number of generated tokens in chat completion
- **`n`** (integer): Generates a specified number of chat completion choices for each input
- **`presence_penalty`** (number): Penalizes new tokens based on their presence in the text. Range: -2.0 to 2.0
- **`response_format`** (object): Specifies the output format, e.g., JSON mode
- **`seed`** (integer): Ensures deterministic sampling with a specified seed
- **`stop`** (string or array): Specifies up to 4 sequences where the API should stop generating tokens
- **`stream`** (boolean): Sends partial message deltas as tokens become available
- **`stream_options`** (object): Options for streaming responses, e.g., `{"include_usage": true}`
- **`temperature`** (number): Sets the sampling temperature between 0 and 2
- **`top_p`** (number): Uses nucleus sampling; considers tokens with top_p probability mass
- **`tools`** (array): Lists functions the model may call
- **`tool_choice`** (string or object): Controls the model's function calls (none/auto/function)
- **`user`** (string): Unique identifier for end-user monitoring and abuse detection

### Message Object Structure

Messages can also contain an optional `name` field, which gives the messenger a name (e.g., `example-user`, `Alice`, `BlackbeardBot`). Names may not contain spaces.

```json
{
  "role": "user",
  "content": "Hello, how are you?",
  "name": "Alice"
}
```

## Request Examples

### Basic Chat Completion

```json
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello, how are you?"}
  ],
  "temperature": 0.7
}
```

### Conversation with History

```json
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Knock knock."},
    {"role": "assistant", "content": "Who's there?"},
    {"role": "user", "content": "Orange."}
  ],
  "temperature": 0
}
```

### Streaming Request

```json
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {"role": "user", "content": "Count to 10"}
  ],
  "stream": true,
  "stream_options": {"include_usage": true}
}
```

## Response Format

### Non-Streaming Response

```json
{
  "id": "chatcmpl-8dee9DuEFcg2QILtT2a6EBXZnpirM",
  "object": "chat.completion",
  "created": 1704461729,
  "model": "gpt-3.5-turbo-0613",
  "system_fingerprint": "fp_e9b8ed65d2",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! I'm doing well, thank you for asking. How can I help you today?",
        "function_call": null,
        "tool_calls": null
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 35,
    "completion_tokens": 15,
    "total_tokens": 50
  }
}
```

### Response Fields

- **`id`**: Unique identifier for the request
- **`object`**: Type of object returned (`chat.completion` for non-streaming, `chat.completion.chunk` for streaming)
- **`created`**: Unix timestamp of when the completion was created
- **`model`**: The model used to generate the response
- **`system_fingerprint`**: Backend configuration fingerprint
- **`choices`**: Array of completion choices
  - **`index`**: Index of the choice in the list
  - **`message`**: The message object generated by the model (non-streaming)
  - **`delta`**: The message delta object (streaming only)
  - **`logprobs`**: Log probability information
  - **`finish_reason`**: Reason the model stopped generating (`stop`, `length`, `function_call`, `tool_calls`, `content_filter`)
- **`usage`**: Token usage statistics
  - **`prompt_tokens`**: Number of tokens in the prompt
  - **`completion_tokens`**: Number of tokens in the completion
  - **`total_tokens`**: Total number of tokens used

## Streaming Responses

When `stream=true`, the response is sent as Server-Sent Events (SSE). Each chunk has the following format:

```
data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"content":" there"},"finish_reason":null}]}

data: {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1677652288,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

### Streaming Delta Object

The `delta` object contains incremental updates:

- **First chunk**: Usually contains the role (`{"role": "assistant"}`)
- **Content chunks**: Contain content tokens (`{"content": "Hello"}`)
- **Final chunk**: Empty delta with `finish_reason` set
- **Usage chunk** (if `stream_options.include_usage=true`): Contains usage statistics with empty choices array

### Streaming with Usage Statistics

When `stream_options={"include_usage": true}` is set:

1. All chunks except the last will have `usage: null`
2. The final chunk contains usage statistics in the `usage` field
3. The final chunk has an empty `choices` array

```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1677652288,
  "model": "gpt-3.5-turbo",
  "choices": [],
  "usage": {
    "prompt_tokens": 18,
    "completion_tokens": 2,
    "total_tokens": 20
  }
}
```

## Error Responses

Error responses follow this format:

```json
{
  "error": {
    "message": "Invalid request: missing required parameter 'model'",
    "type": "invalid_request_error",
    "param": "model",
    "code": "missing_required_parameter"
  }
}
```

Common error types:
- `invalid_request_error`: Invalid request parameters
- `authentication_error`: Invalid API key
- `permission_error`: Insufficient permissions
- `not_found_error`: Resource not found
- `rate_limit_error`: Rate limit exceeded
- `api_error`: Server error
- `overloaded_error`: Server overloaded

## Token Counting

For billing and rate limiting, you need to count tokens accurately. The token count includes:

1. **Message formatting tokens**: Each message adds ~3-4 tokens for formatting
2. **Content tokens**: Actual content encoded using the model's tokenizer
3. **Name tokens**: If a message has a `name` field, it adds ~1 token
4. **Priming tokens**: ~3 tokens are added to prime the assistant's response

### Token Counting Formula (Approximate)

```
total_tokens = 3  # priming tokens
for message in messages:
    total_tokens += 3  # message formatting
    total_tokens += len(encode(message.content))
    if message.name:
        total_tokens += 1
```

## Implementation Notes

### Content Moderation

When implementing streaming responses, consider that partial completions may be harder to moderate than complete responses. Implement appropriate content filtering for your use case.

### Rate Limiting

Implement rate limiting based on:
- Requests per minute/hour
- Tokens per minute/hour
- Concurrent requests

### Model Support

Different models have different capabilities:
- Context length limits (4K, 8K, 16K, 32K, 128K tokens)
- Function calling support
- JSON mode support
- Vision capabilities (for multimodal models)

### Best Practices

1. **System Messages**: Use system messages to set the assistant's behavior
2. **Few-shot Examples**: Include example conversations to guide the model
3. **Temperature Control**: Use lower temperatures (0-0.3) for factual tasks, higher (0.7-1.0) for creative tasks
4. **Token Management**: Monitor token usage to control costs and latency
5. **Error Handling**: Implement robust error handling and retry logic
6. **Streaming UX**: For streaming, display partial responses to improve perceived performance

## Security Considerations

1. **Input Validation**: Validate all input parameters
2. **Content Filtering**: Implement content moderation for both inputs and outputs
3. **Rate Limiting**: Prevent abuse with appropriate rate limits
4. **Authentication**: Secure API access with proper authentication
5. **Logging**: Log requests for monitoring and debugging (excluding sensitive content)

## Compatibility Notes

This API specification is compatible with OpenAI's Chat Completions API v1. Clients using the official OpenAI SDK should work without modification by changing only the base URL and API key.